{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.What is Statistical Machine Translation (SMT)?\n",
    "\n",
    "Statistical Machine Translation (SMT) is a method of machine translation that uses statistical models to translate text from one language to another. It relies on large amounts of bilingual text data (corpora) to build probabilistic models that determine the most likely translation for a given sentence or phrase. SMT works by analyzing word or phrase alignments between languages and using these alignments to generate translations. Unlike rule-based systems, SMT does not require linguistic rules but instead learns patterns from data, improving over time with more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.V What are the main differences between SMT and Neural Machine Translation (NMT)?\n",
    "Model Type:\n",
    "\n",
    "SMT: Uses probabilistic models based on statistical patterns and word alignments between languages.\n",
    "NMT: Uses deep learning models, specifically neural networks, to learn end-to-end translation directly from data.\n",
    "Translation Approach:\n",
    "\n",
    "SMT: Translates text by breaking it into smaller parts (e.g., words or phrases) and applying statistical rules.\n",
    "NMT: Translates text as a whole using a continuous representation of the entire sentence, leading to more fluent translations.\n",
    "Data Dependency:\n",
    "\n",
    "SMT: Relies on large parallel corpora and explicit word alignments for translation.\n",
    "NMT: Requires vast amounts of parallel text but leverages deep learning to automatically learn context and translation patterns.\n",
    "Context Handling:\n",
    "\n",
    "SMT: Struggles with long-range dependencies and context across entire sentences.\n",
    "NMT: Captures broader context and dependencies, improving translation quality, especially for longer texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Explain the concept of attention in Neural Machine Translation?\n",
    "\n",
    "The concept of attention in Neural Machine Translation (NMT) refers to a mechanism that allows the model to focus on different parts of the input sentence while translating each word in the output sentence. Instead of processing the entire sentence in a fixed manner (as in traditional models), attention dynamically weights the importance of each input word for generating the current output word. This helps the model capture long-range dependencies and context, improving translaHow do Generative Pre-trained Transformers (GPTs) contribute to machine translationAtion accuracy, especially for longer sentences. Attention allows the model to \"attend\" to relevant words in the input when producing the corresponding words in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.How do Generative Pre-trained Transformers (GPTs) contribute to machine translation?\n",
    "Pre-training on Large Datasets: GPT models are pre-trained on massive amounts of text data in multiple languages, learning to predict the next word in a sequence. This helps the model grasp grammar, vocabulary, and context across different languages, which is essential for translation tasks.\n",
    "\n",
    "Contextual Understanding: Unlike traditional models, GPTs consider the entire context of a sentence, enabling them to handle longer, more complex sentences and produce more coherent translations.\n",
    "\n",
    "Zero-shot and Few-shot Translation: GPT models, especially advanced versions like GPT-3, can perform translation tasks with little to no task-specific training. This is due to their extensive pre-training on diverse multilingual data, allowing them to generate translations for languages they've never explicitly seen before (zero-shot translation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.What is poetry generation in generative AI?\n",
    "Creativity: AI models generate novel poetic content by predicting and composing words and phrases based on learned patterns, often producing creative and original verses.\n",
    "\n",
    "Style and Structure: AI can mimic various poetic styles (e.g., sonnets, haikus, free verse) and maintain poetic structures, like rhyme schemes and syllable counts.\n",
    "\n",
    "Emotional and Thematic Content: Some models are trained to generate poems that convey specific emotions or themes (e.g., love, nature, melancholy).\n",
    "\n",
    "Applications: Poetry generation can be used for creative writing, entertainment, artistic expression, or even as a tool for inspiring human poets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.How does music composition with generative AI work?\n",
    "Training: The AI is trained on a large collection of music (e.g., classical, jazz, pop) to learn musical patterns and structures.\n",
    "Generation: Once trained, the AI can generate new music by sampling from learned patterns, creating melodies, rhythms, and harmonies that resemble the style of the training data.\n",
    "Control: Some models allow users to input parameters like genre, mood, or instruments to guide the composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.V What role does reinforcement learning play in generative AI for NLP?\n",
    "Reward-based Optimization: RL allows the model to generate text based on rewards (e.g., grammatical correctness, coherence, or relevance), improving its ability to produce desirable outputs.\n",
    "Fine-Tuning: Pre-trained language models, such as GPT, are fine-tuned using RL to make the generated text more contextually appropriate and aligned with human preferences.\n",
    "Exploration and Improvement: RL helps the model explore different responses and gradually refine its behavior to generate more natural, fluent, and engaging language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.What are multimodal generative models?\n",
    "Cross-modal Understanding: They can understand and generate content across various data types, enabling tasks like image captioning, visual question answering, and text-to-image generation.\n",
    "Unified Learning: The model is trained to learn patterns that span different types of data (e.g., text and images together), enhancing its ability to generate more coherent and contextually relevant outputs.\n",
    "Examples: DALLÂ·E (text-to-image generation) and CLIP (relating images and text for better understanding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.Define Natural Language Understanding (NLU) in the context of generative AI\n",
    "Natural Language Understanding (NLU) in the context of generative AI refers to the ability of an AI model to comprehend and interpret human language, extracting meaning from text or speech. NLU enables the model to understand the intent, context, and nuances of language, which is essential for tasks like answering questions, generating coherent responses, and performing language-based tasks such as sentiment analysis or named entity recognition.\n",
    "\n",
    "In short, NLU allows generative AI to \"understand\" human language, facilitating more accurate and contextually relevant generation of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.What ethical considerations arise in generative AI for creative writing?\n",
    "Authorship and Plagiarism: Determining who owns the content generated by AI and ensuring the work isn't inadvertently copied from existing texts.\n",
    "Bias: AI models may reflect and perpetuate biases present in training data, leading to biased or harmful content.\n",
    "Originality: AI-generated content may lack true creativity, raising questions about the value and originality of machine-produced works.\n",
    "Job Displacement: The potential for AI to replace human writers in certain areas, affecting employment in creative industries.\n",
    "Misuse: AI-generated content can be used to create misleading or harmful narratives, such as deepfakes or propaganda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. How can attention mechanisms improve NMT performance on longer sentences?\n",
    "Attention mechanisms improve Neural Machine Translation (NMT) performance on longer sentences by allowing the model to focus on different parts of the input sentence at each step of the translation process.\n",
    "\n",
    "Instead of relying on a fixed-length context (like in traditional models), attention dynamically weights the importance of each word in the input sequence, enabling the model to retain relevant information over longer distances. This helps preserve context and meaning, reducing errors and improving translation accuracy, especially when translating long sentences with complex dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.What are some challenges with bias in generative AI for machine translation?\n",
    "Cultural Bias: AI models may translate phrases or concepts in ways that reflect cultural assumptions, misrepresenting or oversimplifying the meaning.\n",
    "Gender Bias: Machine translation systems may produce gender-biased translations, reinforcing stereotypes (e.g., translating professions to specific genders).\n",
    "Linguistic Bias: Biases in training data may lead to inaccurate translations for certain languages or dialects that are underrepresented.\n",
    "Reinforcing Harmful Content: If the model is trained on biased data, it may perpetuate harmful stereotypes or misinformation in its translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13.Explain how reinforcement learning differs from supervised learning in generative AI\n",
    "Learning Approach:\n",
    "\n",
    "Supervised Learning: The model learns from labeled data (input-output pairs), where the correct answer is provided, and the goal is to minimize error based on this data.\n",
    "Reinforcement Learning: The model learns through trial and error, receiving feedback in the form of rewards or penalties from its actions, with no labeled output. The goal is to maximize long-term rewards.\n",
    "Feedback:\n",
    "\n",
    "Supervised Learning: Feedback is immediate and explicit, based on the correct labels in the training data.\n",
    "Reinforcement Learning: Feedback is delayed and comes from the environment after a series of actions, which can be less direct.\n",
    "Task Type:\n",
    "\n",
    "Supervised Learning: Typically used for tasks like classification, regression, and translation, where the desired outcome is known.\n",
    "Reinforcement Learning: Often used for decision-making tasks (e.g., game playing, robotics) where the model must explore to discover optimal strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14.What is the role of a decoder in NMT models?\n",
    "Context Utilization: It takes the context vector (or attention weights) generated by the encoder to produce each word in the translated sentence.\n",
    "Word Prediction: The decoder generates one word at a time, using the previous word (or token) and the encoder's context to predict the next word.\n",
    "Sequence Generation: It continues generating words until the end of the sequence is reached (e.g., an \"end-of-sentence\" token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.How does fine-tuning a GPT model differ from pre-training it?\n",
    "Objective:\n",
    "Pre-training: The model is trained on a large, diverse dataset to learn general language patterns, grammar, and facts. It learns to predict the next word in a sequence across broad domains.\n",
    "Fine-tuning: The pre-trained model is further trained on a smaller, task-specific dataset to adapt it for a particular application (e.g., translation, question answering, or sentiment analysis).\n",
    "    \n",
    "Data:\n",
    "Pre-training: Uses vast amounts of unstructured data from a wide range of sources.\n",
    "Fine-tuning: Uses labeled or domain-specific data tailored to the desired task.\n",
    "\n",
    "Training Scope:\n",
    "Pre-training: Involves training from scratch on a large-scale dataset, taking a lot of computational power and time.\n",
    "Fine-tuning: Involves adjusting the model's parameters based on the task-specific data, which is generally faster and requires fewer resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.Describe one approach generative AI uses to avoid overfitting in creative content generation?\n",
    "Dropout randomly \"drops\" (or disables) a certain percentage of neurons during training, forcing the model to rely on different parts of the network and preventing it from becoming too reliant on specific features or patterns in the training data.\n",
    "This prevents the model from memorizing the training data (overfitting), encouraging it to learn more generalized features, which is particularly important in creative tasks like text or music generation, where novelty and variety are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17,.What makes GPT-based models effective for creative storytelling?\n",
    "Contextual Understanding: GPT models can generate coherent narratives by understanding and maintaining context over long passages, ensuring a logical flow in the story.\n",
    "Vast Knowledge Base: Trained on extensive datasets, they have knowledge of diverse themes, styles, and genres, enabling them to craft rich, varied stories.\n",
    "Adaptability: GPT models can adapt to different writing styles, tones, and genres based on user prompts, providing flexible and personalized storytelling.\n",
    "Creativity: They generate novel ideas, characters, and plots by predicting and combining words in creative, unexpected ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.How does context preservation work in NMT models?\n",
    "Attention Mechanism: It helps the model focus on relevant parts of the input sentence during translation, allowing it to maintain context across different parts of the sentence. The model can give varying levels of attention to words, ensuring important contextual information is preserved in the output.\n",
    "\n",
    "Encoder-Decoder Structure: The encoder processes the entire input sentence, creating a context vector (or sequence of vectors) that captures the overall meaning. The decoder then uses this context to generate the translation, maintaining coherence and context throughout the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#practical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence: Hello, how are you?\n",
      "Translated Sentence: hello, cÃ³mo estÃ¡n you?\n"
     ]
    }
   ],
   "source": [
    "#1.V Implement a basic Statistical Machine Translation (SMT) model that uses word-by-word translation with a\n",
    "#dictionary lookup approach?\n",
    "\n",
    "# Example dictionary of word translations (source language -> target language)\n",
    "translation_dict = {\n",
    "    'hello': 'hola',\n",
    "    'world': 'mundo',\n",
    "    'good': 'bueno',\n",
    "    'morning': 'maÃ±ana',\n",
    "    'how': 'cÃ³mo',\n",
    "    'are': 'estÃ¡n',\n",
    "    'you': 'tÃº'\n",
    "}\n",
    "\n",
    "\n",
    "def translate_sentence(source_sentence):\n",
    "\n",
    "    source_words = source_sentence.lower().split()\n",
    "\n",
    "    \n",
    "    translated_words = []\n",
    "    for word in source_words:\n",
    "      \n",
    "        translated_word = translation_dict.get(word, word)\n",
    "        translated_words.append(translated_word)\n",
    "\n",
    "    \n",
    "    translated_sentence = ' '.join(translated_words)\n",
    "    return translated_sentence\n",
    "\n",
    "\n",
    "source_sentence = \"Hello, how are you?\"\n",
    "translated_sentence = translate_sentence(source_sentence)\n",
    "\n",
    "print(f\"Source Sentence: {source_sentence}\")\n",
    "print(f\"Translated Sentence: {translated_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #2.Implement an Attention mechanism in a Neural Machine Translation (NMT) model using PyTorch?\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Define the Attention Mechanism\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "#     def forward(self, decoder_hidden, encoder_outputs):\n",
    "#         # Compute the attention scores (alignment scores)\n",
    "#         energy = torch.tanh(self.attn(encoder_outputs))  # Apply a linear transformation\n",
    "#         attention_scores = torch.bmm(energy, decoder_hidden.unsqueeze(2))  # Compute dot product with decoder hidden state\n",
    "#         attention_weights = torch.softmax(attention_scores.squeeze(2), dim=1)  # Apply softmax to get attention weights\n",
    "#         return attention_weights\n",
    "\n",
    "# # Define the Encoder\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "    \n",
    "#     def forward(self, input_seq, hidden=None):\n",
    "#         embedded = self.embedding(input_seq)  # Embed the input\n",
    "#         outputs, (hidden, cell) = self.rnn(embedded, hidden)  # Get LSTM output\n",
    "#         return outputs, (hidden, cell)\n",
    "\n",
    "# # Define the Decoder\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, output_size, hidden_size, attention, n_layers=1):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.attn = attention\n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "#     def forward(self, input_seq, hidden, encoder_outputs):\n",
    "#         embedded = self.embedding(input_seq)  # Embed the input\n",
    "#         output, (hidden, cell) = self.rnn(embedded, hidden)  # Get LSTM output\n",
    "        \n",
    "#         # Compute attention weights\n",
    "#         attention_weights = self.attn(output, encoder_outputs)  # Get attention weights from the mechanism\n",
    "#         context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # Compute weighted sum of encoder outputs\n",
    "        \n",
    "#         output = self.out(output.squeeze(1))  # Predict the next token\n",
    "#         return output, hidden, attention_weights\n",
    "\n",
    "# # Define the Seq2Seq Model with Attention\n",
    "# class Seq2Seq(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, hidden_size, attention, n_layers=1):\n",
    "#         super(Seq2Seq, self).__init__()\n",
    "#         self.encoder = Encoder(input_size, hidden_size, n_layers)\n",
    "#         self.decoder = Decoder(output_size, hidden_size, attention, n_layers)\n",
    "    \n",
    "#     def forward(self, input_seq, target_seq):\n",
    "#         encoder_outputs, hidden = self.encoder(input_seq)  # Get encoder outputs and hidden states\n",
    "        \n",
    "#         decoder_input = target_seq[:, 0]  # Start with the first token of the target sequence\n",
    "#         decoder_hidden = hidden  # Initialize decoder hidden state\n",
    "        \n",
    "#         outputs = []\n",
    "#         for t in range(1, target_seq.size(1)):  # Loop over the target sequence length\n",
    "#             decoder_output, decoder_hidden, attention_weights = self.decoder(decoder_input.unsqueeze(1), decoder_hidden, encoder_outputs)\n",
    "#             outputs.append(decoder_output)\n",
    "#             decoder_input = target_seq[:, t]  # Use the next token in the sequence as the input to the decoder\n",
    "        \n",
    "#         outputs = torch.stack(outputs, dim=1)  # Stack outputs for all time steps\n",
    "#         return outputs\n",
    "\n",
    "# # Hyperparameters\n",
    "# input_size = 10  # Example input vocabulary size\n",
    "# output_size = 10  # Example output vocabulary size\n",
    "# hidden_size = 64  # Hidden size for LSTM\n",
    "# n_layers = 1  # Number of layers in the encoder/decoder\n",
    "\n",
    "# # Instantiate the attention mechanism\n",
    "# attention = Attention(hidden_size)\n",
    "\n",
    "# # Instantiate the Seq2Seq model with attention\n",
    "# model = Seq2Seq(input_size, output_size, hidden_size, attention, n_layers)\n",
    "\n",
    "# # Example input and target sequences (batch_size=1, seq_len=5)\n",
    "# input_seq = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long)  # Example input sequence\n",
    "# target_seq = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long)  # Example target sequence\n",
    "\n",
    "# # Forward pass through the model\n",
    "# output = model(input_seq, target_seq)\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#3.Use a pre-trained GPT model to perform machine translation from English to French?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2LMHeadModel, GPT2Tokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load GPT2 model and tokenizer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#3.Use a pre-trained GPT model to perform machine translation from English to French?\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text (not a translation, just for demonstration)\n",
    "input_text = \"Translate this English text to French: Hello, how are you today?\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response (this is not guaranteed to be a proper translation)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.2.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 3.9/203.0 MB 19.5 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 10.5/203.0 MB 26.1 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 17.0/203.0 MB 27.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 20.4/203.0 MB 24.8 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 30.7/203.0 MB 29.0 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 48.0/203.0 MB 30.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 53.5/203.0 MB 30.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 60.6/203.0 MB 31.1 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 71.0/203.0 MB 31.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 80.5/203.0 MB 32.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 92.0/203.0 MB 34.5 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 102.5/203.0 MB 35.7 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 114.0/203.0 MB 37.0 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 125.3/203.0 MB 37.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 135.5/203.0 MB 38.1 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 143.1/203.0 MB 38.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 149.9/203.0 MB 37.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 160.4/203.0 MB 38.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 169.3/203.0 MB 38.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 177.7/203.0 MB 38.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 185.3/203.0 MB 38.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 193.5/203.0 MB 38.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.1/203.0 MB 37.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 37.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 37.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 37.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 37.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.0/203.0 MB 31.4 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 13.1 MB/s eta 0:00:00\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, torch\n",
      "Successfully installed fsspec-2024.10.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 7.3/10.1 MB 28.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 27.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.5 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.47.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ram sharma\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short poem about Nature:\n",
      "\n",
      "The poem is a poem of the nature of Nature.\n",
      ". . .\n",
      " (The poet is the poet, and the poem the writer.)\n",
      ", . (the poet and writer. The poem and its author.) The poet's poem. (A poem, a writer, or a poet.) A poem that is not a poetry. A poet who is an artist. An artist who has a work of art. Artist who works on a piece of\n"
     ]
    }
   ],
   "source": [
    "#4.Generate a short poem using GPT-2 for a specific theme (e.g., \"Nature\")?\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the theme for the poem\n",
    "theme = \"Nature\"\n",
    "prompt = f\"Write a short poem about {theme}:\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text from the model\n",
    "outputs = model.generate(inputs, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated poem\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Generated Text: Once upon a time, the world will be a better.\n",
      "\n",
      "The world will be a better world.\n",
      "\n",
      "The world will be a better world.\n",
      "\n",
      "\n",
      "The world will be a better world.\n",
      "\n",
      "The world will be a, Reward: 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Generated Text: Once upon a time, a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a, Reward: 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Generated Text: Once upon a time, a a a a a a, a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a, Reward: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Generated Text: Once upon a time a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a, Reward: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Generated Text: Once upon a time a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a, Reward: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Generated Text: Once upon a time a time a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a, Reward: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Generated Text: Once upon a time a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a, Reward: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Generated Text: Once upon a time a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a, Reward: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Generated Text: Once upon a time a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a, Reward: 108\n",
      "Epoch 10: Generated Text: Once upon a time a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a, Reward: 108\n"
     ]
    }
   ],
   "source": [
    "#5.Implement a basic reinforcement learning setup for text generation using PyTorch's reward function?\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import random\n",
    "\n",
    "# Define a simple reward function (for demonstration)\n",
    "def reward_function(text):\n",
    "    # In this example, we reward longer sentences (basic length-based reward)\n",
    "    return len(text)\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Policy Gradient for text generation (simplified)\n",
    "def generate_text(prompt, max_length=50):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def reinforce_learning_step(prompt):\n",
    "    model.train()\n",
    "    generated_text = generate_text(prompt)\n",
    "    \n",
    "    # Calculate reward (based on length in this example)\n",
    "    reward = reward_function(generated_text)\n",
    "    \n",
    "    # Tokenize the generated text and compute the loss\n",
    "    inputs = tokenizer.encode(generated_text, return_tensors=\"pt\")\n",
    "    labels = inputs.clone()  # In RL, labels are the same as inputs\n",
    "    \n",
    "    # Perform a forward pass and calculate the loss\n",
    "    outputs = model(input_ids=inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Compute gradients using policy gradient method\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return generated_text, reward\n",
    "\n",
    "# Training loop for RL\n",
    "prompt = \"Once upon a time\"\n",
    "for epoch in range(10):\n",
    "    generated_text, reward = reinforce_learning_step(prompt)\n",
    "    print(f\"Epoch {epoch+1}: Generated Text: {generated_text}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ram sharma\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Ram sharma/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "  7%|â         | 7.25M/97.8M [00:00<00:02, 35.5MB/s]"
     ]
    }
   ],
   "source": [
    "#6.Create a simple multimodal generative model that generates an image caption given an image\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Define the CNN (ResNet50) model for feature extraction from images\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet50 for image feature extraction\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove the classification layer\n",
    "        \n",
    "        # Define the LSTM for generating the caption\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to map the LSTM output to vocabulary space\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        # Extract image features using ResNet50\n",
    "        image_features = self.resnet(images)\n",
    "        \n",
    "        # Pass image features through the LSTM to generate captions\n",
    "        lstm_out, _ = self.lstm(image_features.unsqueeze(1), captions)\n",
    "        \n",
    "        # Predict the next word in the caption using the fully connected layer\n",
    "        outputs = self.fc(lstm_out)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Load the BERT tokenizer for word embedding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to process an image\n",
    "def preprocess_image(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# Example of how the multimodal model works\n",
    "def generate_caption(image, model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the image\n",
    "    image_tensor = preprocess_image(image).to(device)\n",
    "    \n",
    "    # Generate the caption (simplified)\n",
    "    with torch.no_grad():\n",
    "        features = model.resnet(image_tensor)  # Get image features\n",
    "        caption_tensor = torch.tensor([[tokenizer.cls_token_id]])  # Start with [CLS] token\n",
    "        caption_tensor = caption_tensor.to(device)\n",
    "        \n",
    "        # Run through LSTM and generate words\n",
    "        lstm_out, _ = model.lstm(features.unsqueeze(1), caption_tensor)\n",
    "        \n",
    "        # Convert to word index and decode to text\n",
    "        predicted_token = torch.argmax(model.fc(lstm_out.squeeze(1)), dim=1)\n",
    "        caption = tokenizer.decode(predicted_token[0])\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Initialize the model (using 512 as an example for embedding and hidden dimensions)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageCaptioningModel(embedding_dim=512, hidden_dim=512, vocab_size=len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# Example image (replace with an actual image)\n",
    "from PIL import Image\n",
    "image_path = \"path_to_your_image.jpg\"  # Change this to a valid image path\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Generate caption for the image\n",
    "caption = generate_caption(image, model, device)\n",
    "print(f\"Generated Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ram sharma\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 21.0 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
